services:
  # Model initialization - downloads GGUF model on first run
  model-init:
    image: python:3.11-slim
    container_name: dendrite-model-init
    volumes:
      - models:/models
      - ./scripts/model-init.sh:/init.sh:ro
    environment:
      RAM_PROFILE: "${RAM_PROFILE:-16gb}"
    entrypoint: ["/bin/bash", "/init.sh"]
    profiles:
      - cpu
      - gpu

  # llama.cpp server (CPU)
  llama-cpu:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: dendrite-llama-cpu
    ports:
      - "8080:8080"
    volumes:
      - models:/models
    environment:
      LLAMA_ARG_MODEL: /models/current.gguf
      LLAMA_ARG_CTX_SIZE: "4096"
      LLAMA_ARG_N_GPU_LAYERS: "0"
      LLAMA_ARG_THREADS: "8"
      LLAMA_ARG_HOST: "0.0.0.0"
      LLAMA_ARG_PORT: "8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      model-init:
        condition: service_completed_successfully
    profiles:
      - cpu

  # llama.cpp server (GPU)
  llama-gpu:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: dendrite-llama-gpu
    ports:
      - "8080:8080"
    volumes:
      - models:/models
    environment:
      LLAMA_ARG_MODEL: /models/current.gguf
      LLAMA_ARG_CTX_SIZE: "8192"
      LLAMA_ARG_N_GPU_LAYERS: "999"
      LLAMA_ARG_HOST: "0.0.0.0"
      LLAMA_ARG_PORT: "8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      model-init:
        condition: service_completed_successfully
    profiles:
      - gpu

  app:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app  # Mount entire source including tools
      - kv_store:/app/var
    environment:
      - REDIS_HOST=redis
      - OPENAI_API_BASE=http://llama-cpu:8080/v1
      - OPENAI_API_KEY=not-needed
      - LLM_MODEL=local-model
    depends_on:
      redis:
        condition: service_started
      llama-cpu:
        condition: service_healthy
    profiles:
      - cpu

  app-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app
      - kv_store:/app/var
    environment:
      - REDIS_HOST=redis
      - OPENAI_API_BASE=http://llama-gpu:8080/v1
      - OPENAI_API_KEY=not-needed
      - LLM_MODEL=local-model
    depends_on:
      redis:
        condition: service_started
      llama-gpu:
        condition: service_healthy
    profiles:
      - gpu

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --appendfsync everysec
    # appendonly yes = AOF persistence (survives restarts)
    # appendfsync everysec = sync every second (good balance of safety/performance)

  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: dendrite
      POSTGRES_USER: dendrite
      POSTGRES_PASSWORD: dendrite_pass
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/db/init_db.sql:/docker-entrypoint-initdb.d/01_init_db.sql
      - ./scripts/db/init_extensions.sql:/docker-entrypoint-initdb.d/02_init_extensions.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dendrite"]
      interval: 5s
      timeout: 5s
      retries: 5

  # HTTP API server (CPU)
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: dendrite-api
    ports:
      - "8000:8000"
    volumes:
      - .:/app
      - kv_store:/app/var
      - huggingface_cache:/root/.cache/huggingface
      - chroma_cache:/root/.cache/chroma
      - chroma_data:/app/chroma_data
    environment:
      REDIS_HOST: redis
      OPENAI_API_BASE: http://llama-cpu:8080/v1
      OPENAI_API_KEY: not-needed
      LLM_MODEL: local-model
      POSTGRES_HOST: postgres
      POSTGRES_DB: dendrite
      POSTGRES_USER: dendrite
      POSTGRES_PASSWORD: dendrite_pass
      DENDRITE_LOG_FORMAT: json
      DENDRITE_LOG_LEVEL: INFO
    depends_on:
      redis:
        condition: service_started
      llama-cpu:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    command: ["python", "-m", "uvicorn", "neural_engine.api.server:app", "--host", "0.0.0.0", "--port", "8000"]
    profiles:
      - api

  # HTTP API server (GPU)
  api-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: dendrite-api-gpu
    ports:
      - "8000:8000"
    volumes:
      - .:/app
      - kv_store:/app/var
      - huggingface_cache:/root/.cache/huggingface
      - chroma_cache:/root/.cache/chroma
      - chroma_data:/app/chroma_data
    environment:
      REDIS_HOST: redis
      OPENAI_API_BASE: http://llama-gpu:8080/v1
      OPENAI_API_KEY: not-needed
      LLM_MODEL: local-model
      POSTGRES_HOST: postgres
      POSTGRES_DB: dendrite
      POSTGRES_USER: dendrite
      POSTGRES_PASSWORD: dendrite_pass
      DENDRITE_LOG_FORMAT: json
      DENDRITE_LOG_LEVEL: INFO
    depends_on:
      redis:
        condition: service_started
      llama-gpu:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    command: ["python", "-m", "uvicorn", "neural_engine.api.server:app", "--host", "0.0.0.0", "--port", "8000"]
    profiles:
      - gpu-api

  tests:
    build:
      context: .
      dockerfile: Dockerfile.test
    depends_on:
      redis:
        condition: service_started
      postgres:
        condition: service_healthy
    environment:
      REDIS_HOST: redis
      # Default to llama-gpu (override with OPENAI_API_BASE env var if needed)
      OPENAI_API_BASE: ${OPENAI_API_BASE:-http://llama-gpu:8080/v1}
      OPENAI_API_KEY: not-needed
      LLM_MODEL: local-model
      POSTGRES_HOST: postgres
      POSTGRES_DB: dendrite
      POSTGRES_USER: dendrite
      POSTGRES_PASSWORD: dendrite_pass
    volumes:
      - .:/app
      - huggingface_cache:/root/.cache/huggingface  # Cache SentenceTransformer models
      - chroma_cache:/root/.cache/chroma  # Cache ChromaDB ONNX models
      - chroma_data:/app/chroma_data  # Persist Chroma vector database
    command: pytest
    # No profile - works with any backend

  tests-gpu:
    build:
      context: .
      dockerfile: Dockerfile.test
    depends_on:
      redis:
        condition: service_started
      llama-gpu:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      REDIS_HOST: redis
      OPENAI_API_BASE: http://llama-gpu:8080/v1
      OPENAI_API_KEY: not-needed
      LLM_MODEL: local-model
      POSTGRES_HOST: postgres
      POSTGRES_DB: dendrite
      POSTGRES_USER: dendrite
      POSTGRES_PASSWORD: dendrite_pass
    volumes:
      - .:/app
      - huggingface_cache:/root/.cache/huggingface  # Cache SentenceTransformer models
      - chroma_cache:/root/.cache/chroma  # Cache ChromaDB ONNX models
      - chroma_data:/app/chroma_data  # Persist Chroma vector database
    command: pytest
    profiles:
      - gpu

volumes:
  redis_data:
  postgres_data:
  models:
    name: dendrite-models
  kv_store:
  huggingface_cache:  # Persist SentenceTransformer model downloads (90MB)
  chroma_cache:  # Persist ChromaDB ONNX models downloads (79MB)
  chroma_data:  # Persist Chroma vector database (neural pathways)
